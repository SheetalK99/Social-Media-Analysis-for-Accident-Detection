{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    " import spotlight\n",
    "annotations = spotlight.annotate('http://api.dbpedia-spotlight.org/en/annotate',\n",
    "                                 'RT @CommuterBoston: ACCIDENT (Wellesley, MA): RT-9 East near Cedar St - involving a motorcycle and a car - possible lane restrictions',\n",
    "                              confidence=0.4, support=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'URI': 'http://dbpedia.org/resource/Sacramento_Regional_Transit_District', 'support': 166, 'types': 'Wikidata:Q43229,Wikidata:Q24229398,DUL:SocialPerson,DUL:Agent,Schema:Organization,DBpedia:Organisation,DBpedia:Company,DBpedia:BusCompany,DBpedia:Agent', 'surfaceForm': 'RT', 'offset': 0, 'similarityScore': 0.6784719207428763, 'percentageOfSecondRank': 0.22935605056432617}, {'URI': 'http://dbpedia.org/resource/Master_of_Arts', 'support': 9411, 'types': '', 'surfaceForm': 'MA', 'offset': 41, 'similarityScore': 0.8409295220849131, 'percentageOfSecondRank': 0.15356639931016167}, {'URI': 'http://dbpedia.org/resource/Sacramento_Regional_Transit_District', 'support': 166, 'types': 'Wikidata:Q43229,Wikidata:Q24229398,DUL:SocialPerson,DUL:Agent,Schema:Organization,DBpedia:Organisation,DBpedia:Company,DBpedia:BusCompany,DBpedia:Agent', 'surfaceForm': 'RT', 'offset': 46, 'similarityScore': 0.6784719207428763, 'percentageOfSecondRank': 0.22935605056432617}, {'URI': 'http://dbpedia.org/resource/Motorcycle', 'support': 9795, 'types': '', 'surfaceForm': 'motorcycle', 'offset': 84, 'similarityScore': 0.9796183031554393, 'percentageOfSecondRank': 0.007411232120353015}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(annotations)\n",
    "\n",
    "lst=[]\n",
    "for ann in annotations:\n",
    "    lst.append(ann['URI'])\n",
    "    \n",
    "lst='ab ab ab cd ef ef'.split()    \n",
    "duplicates(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def duplicates(values):\n",
    "    dups = Counter(values) - Counter(set(values))\n",
    "    return sum(dups.values()) + len(dups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "java_path = \"C:\\\\Program Files\\\\Java\\\\jdk1.8.0_231\\\\bin\\\\java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('While', 'O'), ('in', 'O'), ('France', 'LOCATION'), (',', 'O'), ('Christine', 'PERSON'), ('Lagarde', 'PERSON'), ('discussed', 'O'), ('short-term', 'O'), ('stimulus', 'O'), ('efforts', 'O'), ('in', 'O'), ('a', 'O'), ('recent', 'O'), ('interview', 'O'), ('with', 'O'), ('the', 'O'), ('Wall', 'O'), ('Street', 'O'), ('Journal', 'O'), ('.', 'O')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['France']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "st = StanfordNERTagger('Data/english.all.3class.distsim.crf.ser.gz',\n",
    "\t\t\t\t\t   'Data/stanford-ner.jar',\n",
    "\t\t\t\t\t   encoding='utf-8')\n",
    "\n",
    "text = 'While in France, Christine Lagarde discussed short-term stimulus efforts in a recent interview with the Wall Street Journal.'\n",
    "\n",
    "tokenized_text = word_tokenize(text)\n",
    "classified_text = st.tag(tokenized_text)\n",
    "\n",
    "print(classified_text)\n",
    "\n",
    "location_extracts=[tupl[0] for tupl in classified_text if tupl[1]=='LOCATION']\n",
    "location_extracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'HeidelTime'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-989b625b7948>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mHeidelTime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mhw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mHeidelTime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHeidelTimeWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mhw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Neil Armstrong was born in 1930'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'HeidelTime'"
     ]
    }
   ],
   "source": [
    "import HeidelTime\n",
    "hw = HeidelTime.HeidelTimeWrapper('english')\n",
    "hw.parse('Neil Armstrong was born in 1930')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateparser.search import search_dates\n",
    "\n",
    "dates = search_dates('Central design committee session  11 25 am')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "if isinstance(dates[0][1], datetime.datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def initialize_ngram(corpus):\n",
    "    v1 = CountVectorizer()\n",
    "    v2 = CountVectorizer(2)\n",
    "    \n",
    "    v1.fit(corpus)\n",
    "    v2.fit(corpus)\n",
    "    return v1,v2\n",
    "\n",
    "def convert_ngram_vec(text,v):\n",
    "    text=' '.join(text)\n",
    "    s=[]\n",
    "    s.append(text)\n",
    "    return v.transform(s).toarray()\n",
    "    \n",
    "all_tweets_corpus=boston['tweet'].tolist()\n",
    "\n",
    "v1,v2=initialize_ngram(all_tweets_corpus)\n",
    "\n",
    "boston['unigram_vec'] = boston.apply(lambda row: convert_ngram_vec(row['tweet_tok_lem'],v1), axis=1)\n",
    "\n",
    "boston['bigram_vec'] = boston.apply(lambda row: convert_ngram_vec(row['tweet_tok_lem'],v2), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "a={1,2,3}\n",
    "b={1,2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.intersection(b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
