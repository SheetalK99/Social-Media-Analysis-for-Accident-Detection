# -*- coding: utf-8 -*-
"""BigDataP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Jo0uUzlWyEQvAAQAm8nVWBy-bqMRf3k3
"""

import pandas as pd
import unicodedata
from nltk.corpus import stopwords
import nltk
import re
import math
import spotlight
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.stem import WordNetLemmatizer 
from sklearn.feature_extraction.text import CountVectorizer
from nltk.tag import StanfordNERTagger
from nltk.tokenize import word_tokenize
from io import StringIO
import os
import spacy
from spacy import displacy
from collections import Counter
from pprint import pprint
import en_core_web_sm

java_path = "C:\\Program Files\\Java\\jdk1.8.0_231\\bin\\java.exe"
os.environ['JAVAHOME'] = java_path

nlp = en_core_web_sm.load()
nltk.download('wordnet')
nltk.download("stopwords")
lemmatizer = WordNetLemmatizer()
pd.set_option('display.max_colwidth', -1)

for_pd = StringIO()
with open('Data/2CVTweets/boston2C.csv',encoding="utf8") as f:
    for line in f:
        new_line = re.sub(r'NO;', 'NO',line)
        print (new_line, file=for_pd)

for_pd.seek(0)

df_city = pd.read_csv(for_pd, sep=';', header=None)

#read data of city

#df_city = pd.read_csv('Data/2CVTweets/df_city2C.csv', sep=";",header=None)
df_city = df_city.drop(df_city.columns[0], axis=1) ## index column
df_city.columns=['tweet','Y']


#convert class to upppercase and chang name

df_city.head(20)

"""## Pre-Processing"""

#unicode conversion
df_city['tweet_org']=df_city['tweet']
df_city['tweet'] = (df_city['tweet'].map(lambda x: unicodedata.normalize('NFKD', x))
              .str.encode('ascii', 'ignore'))

# slang conversion

from bs4 import BeautifulSoup
import requests, json
resp = requests.get('http://www.netlingo.com/acronyms.php')
soup = BeautifulSoup(resp.text, "html.parser")
slangdict= {}
key=""
value=""
for div in soup.findAll('div', attrs={'class':'list_box3'}):
    
    for li in div.findAll('li'):
        for a in li.findAll('a'):
            key =a.text
            value = li.text.split(key)[1]
            slangdict[key.lower()]=value

def slang_to_formal(input):
    i=1
    formal=[]
    for slang in input.split():
        i=i+1
        if slang.lower() in slangdict:
            formal.append(slangdict[slang])
        elif slang.isdigit():
            
            formal.append('D')  #not working
        else:
            formal.append(slang)
       
        
    return  " ".join(str(x) for x in formal)

df_city['tweet'] = df_city.apply(lambda row: slang_to_formal(row['tweet']), axis=1)

#replacing URL and digits

def replace_URL(tweet):
    return re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', 'URL', tweet)

    
   


df_city['tweet'] = df_city.apply(lambda row: replace_URL(row['tweet']), axis=1)

#remove stop words

stop = stopwords.words('english')
df_city['tweet'] = df_city.apply(lambda x: [item for item in x if item not in stop])

#tokenisation

df_city['tweet_tok_lem'] = df_city.apply(lambda row: nltk.word_tokenize(row['tweet']), axis=1)

#lemmatize
def lemmatize(s):
       
    s = [lemmatizer.lemmatize(word) for word in s]
    return s

df_city['tweet_tok_lem'] = df_city.apply(lambda row: lemmatize(row['tweet_tok_lem']), axis=1)

"""## Feature Extraction"""

# unigram and bigram features

# freq vector unigram

def initialize_ngram(corpus):
    v1 = CountVectorizer()
    v2 = CountVectorizer(2)
    
    v1.fit(corpus)
    v2.fit(corpus)
    return v1,v2

def convert_ngram_vec(text,v):
    text=' '.join(text)
    s=[]
    s.append(text)
    return v.transform(s).toarray()
    
all_tweets_corpus=df_city['tweet'].tolist()

v1,v2=initialize_ngram(all_tweets_corpus)

df_city['unigram_vec'] = df_city.apply(lambda row: convert_ngram_vec(row['tweet_tok_lem'],v1), axis=1)

df_city['bigram_vec'] = df_city.apply(lambda row: convert_ngram_vec(row['tweet_tok_lem'],v2), axis=1)

#tfidf scores

def compute_num_tokens(tweet_token_list):
    num_tokens = dict.fromkeys(uniqueWords, 0)
    for token in tweet_token_list:
        num_tokens[token] += 1
    return num_tokens

def apply_tfidf(tweet_token_list,idfs):
    num_tokens=compute_num_tokens(tweet_token_list)
    tfDict=computeTF(num_tokens,tweet_token_list)  
    
    return computeTFIDF(tfDict,idfs)

def computeTF(wordDict, bagOfWords):
    tfDict = {}
    bagOfWordsCount = len(bagOfWords)
    for word, count in wordDict.items():
        tfDict[word] = count / float(bagOfWordsCount)
    return tfDict

def computeIDF(documents):

    N = len(documents)
    
    idfDict = dict.fromkeys(documents[0].keys(), 0)
    for document in documents:
        for word, val in document.items():
            if val > 0:
                idfDict[word] += 1
    
    for word, val in idfDict.items():
        idfDict[word] = math.log(N / float(val))
    return idfDict

def computeTFIDF(tfBagOfWords, idfs):
    tfidf = {}
    for word, val in tfBagOfWords.items():
        tfidf[word] = val * idfs[word]
    return tfidf

uniqueWords=set()
for tweet in df_city['tweet_tok_lem']:
    uniqueWords = uniqueWords.union(set(tweet))

idfs = computeIDF([compute_num_tokens(tweet) for tweet in df_city['tweet_tok_lem']])

df_city['tfid'] = df_city.apply(lambda row: apply_tfidf(row['tweet_tok_lem'],idfs), axis=1)

#syntactic features

df_city['no_ques_marks']=df_city.apply(lambda row: (row['tweet'].count("?")), axis=1)
df_city['no_excl_marks']=df_city.apply(lambda row: (row['tweet'].count("!")), axis=1)

df_city['no_uppercase']=df_city.apply(lambda row: sum(map(str.isupper, row['tweet'].split())), axis=1)

df_city[df_city.Y == 'NO']

#Number same URI (NER-DBPedia)

from collections import Counter

def duplicates(values):
    
    dups = Counter(values) - Counter(set(values))
    return sum(dups.values()) + len(dups)

#DBPedia api hit
def get_annotation(tweet):
    try:
        annotations = spotlight.annotate('http://api.dbpedia-spotlight.org/en/annotate',tweet,
                              confidence=0.4, support=20)
    except Exception:
        return {'URI':[],'types':[]}
    
    URI_lst=[]
    types_lst=[]
    
    for ann_dict in annotations:
        URI_lst.append(ann_dict['URI'])
        types_lst.append(ann_dict['types'])
    
    return {'URI':URI_lst,'types':types_lst}

#annotation features

#global 
accident_types=set()
non_accident_types=set()
    
def create_accident_types(ann_types,Y):
  
       
    for types in ann_types:
        if Y=='YES':
            accident_types.add(types)
        else:
            non_accident_types.add(types)
    
   # return accident_types,non_accident_types

df_city['annotations'] = df_city.apply(lambda row: get_annotation(row['tweet_org']), axis=1)

for ann,Y in zip(df_city.annotations, df_city.Y):
    if 'types' in ann:
        create_accident_types(ann['types'],Y)
print(non_accident_types)

df_city['num_same_URI'] = df_city.apply(lambda row: duplicates(row['annotations']['URI']), axis=1)
df_city['acc_overlap_types'] = df_city.apply(lambda row: (len(accident_types.intersection(row['annotations']['types']))/len(accident_types)), axis=1)
df_city['non_acc_overlap_types'] = df_city.apply(lambda row: (len(non_accident_types.intersection(row['annotations']['types']))/len(non_accident_types)), axis=1)

st = StanfordNERTagger('Data/english.all.3class.distsim.crf.ser.gz',
					   'Data/stanford-ner.jar',
					   encoding='utf-8')


def get_location_count(text):
    doc = nlp(text)
  #  pprint([(X.text, X.label_) for X in doc.ents])
   # tokenized_text = word_tokenize(text)
   # classified_text = st.tag(tokenized_text)
    
    
    return sum([1  for X in doc.ents if X.label_=='GPE'])

df_city['num_locations'] = df_city.apply(lambda row: get_location_count(row['tweet']), axis=1)

features = df_city[['unigram_vec','bigram_vec','tfid','no_ques_marks', 'no_excl_marks', 'no_uppercase', 'annotations',
       'num_same_URI', 'acc_overlap_types', 'non_acc_overlap_types',
       'num_locations']]

labels = df_city.loc[:, 'Y']
processed_features=[]

for col in features.columns:
    processed_features.append(features[col].tolist())

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(processed_features, labels, test_size=0.2, random_state=0)

## converting to transform objects

vocabulary = "a list of words I want to look for in the documents".split()
vect = TfidfVectorizer(sublinear_tf=True, max_df=0.5, analyzer='word', 
           stop_words='english', vocabulary=vocabulary)



df_city[['tweet']]



import string
def remove_punctuation(tweet):
    clean = re.sub(r"""
               [,:.;@([#)]?!&$]+  # Accept one or more copies of punctuation
               \ *           # plus zero or more copies of a space,
               """,
               " ",          # and replace it with a single space
               tweet, flags=re.VERBOSE)
    return(clean)
df_city['tweet'] = df_city.apply(lambda row: remove_punctuation(row['tweet']), axis=1)

all_tweets_corpus=df_city['tweet'].tolist()
vocabulary = [item for sublist in all_tweets_corpus for item in sublist.split()]

vect = TfidfVectorizer(sublinear_tf=True, max_df=0.5, analyzer='word', 
           stop_words='english')

vect.fit(all_tweets_corpus)

from sklearn.pipeline import FeatureUnion
from sklearn.decomposition import PCA, TruncatedSVD
union = FeatureUnion([('TfIdf', TfidfVectorizer(min_df=1, max_df=0.9, strip_accents='unicode', norm='l2')),('TfIdf2', TfidfVectorizer(min_df=1, max_df=0.9, ngram_range=(2,2), strip_accents='unicode', norm='l2'))])


union.fit_transform(all_tweets_corpus)

from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
clf = SVC(gamma='scale',kernel='rbf')
#clf = SVR(kernel='linear')
#clf.fit(union, labels)




feature_pipeline = Pipeline([('union', union),('rf',RandomForestClassifier())]) 

p=feature_pipeline.fit(all_tweets_corpus)

from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import TruncatedSVD
from sklearn.ensemble import RandomForestClassifier
#from xgboost import XGBClassifier
from nltk.corpus import stopwords
nltk.download("stopwords")

stop_words = stopwords.words('english')

classifier = Pipeline([
    ('features', FeatureUnion([
        ('tfidf_reduced', Pipeline([
            ('colext', TextSelector('tweet')),
            ('tfidf', TfidfVectorizer(tokenizer=Tokenizer, stop_words=stop_words,
                     min_df=.0025, max_df=0.25, ngram_range=(1,2))),
            ('svd', TruncatedSVD(algorithm='randomized', n_components=300)), #for XGB
        ])),
        ('question_marks', Pipeline([
            ('wordext', NumberSelector('no_ques_marks')),
            ('wscaler', StandardScaler()),
        ])),
         ('no_excl_marks', Pipeline([
            ('wordext', NumberSelector('no_excl_marks')),
            ('wscaler', StandardScaler()),
        ])),
         ('no_uppercase', Pipeline([
            ('wordext', NumberSelector('no_uppercase')),
            ('wscaler', StandardScaler()),
        ])),
         ('num_locations', Pipeline([
            ('wordext', NumberSelector('num_locations')),
            ('wscaler', StandardScaler()),
        ])),
         ('num_same_URI', Pipeline([
            ('wordext', NumberSelector('num_same_URI')),
            ('wscaler', StandardScaler()),
        ])),
         ('acc_overlap_types', Pipeline([
            ('wordext', NumberSelector('acc_overlap_types')),
        
        ])),
         ('non_acc_overlap_types', Pipeline([
            ('wordext', NumberSelector('non_acc_overlap_types')),
           
        ])),
    ])),
#    ('clf', XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.1)),
    ('clf', RandomForestClassifier()),
    ])

from sklearn.base import BaseEstimator, TransformerMixin
class TextSelector(BaseEstimator, TransformerMixin):
    def __init__(self, field):
        self.field = field
    def fit(self, X, y=None):
        return self
    def transform(self, X):
        return X[self.field]
class NumberSelector(BaseEstimator, TransformerMixin):
    def __init__(self, field):
        self.field = field
    def fit(self, X, y=None):
        return self
    def transform(self, X):
        return X[[self.field]]
    
    
import nltk
def Tokenizer(str_input):
    words = re.sub(r"[^A-Za-z0-9\-]", " ", str_input).lower().split()
    porter_stemmer=nltk.PorterStemmer()
   # words = [porter_stemmer.stem(word) for word in words]
    return words

X =df_city[['tweet','unigram_vec','bigram_vec','tfid','no_ques_marks', 'no_excl_marks', 'no_uppercase', 'annotations',
       'num_same_URI', 'acc_overlap_types', 'non_acc_overlap_types',
       'num_locations']]
Y = labels
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25)


classifier.fit(X_train, y_train)
preds = classifier.predict(X_test)

from sklearn.metrics import accuracy_score, precision_score, classification_report, confusion_matrix
print("Accuracy:", accuracy_score(y_test, preds))
#print("Precision:", precision_score(y_test, preds))
print(classification_report(y_test, preds))
print(confusion_matrix(y_test, preds))

from sklearn.metrics import f1_score

f1_score(y_test, preds, average='weighted')